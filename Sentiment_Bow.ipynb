{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Sentiment Analysis on Airline Dataset\n",
    "\n",
    "Authors: Meena Rapaka, Siva Naga Lakshmi Karamsetty, Ying Ke.\n",
    "\n",
    "1.Introduction\n",
    "The project is regarding analysis about the problems of each major U.S airline such as American Airlines, Delta, \n",
    "Southwest, United, US Airways, Virgin America. \n",
    "Twitter data was scraped from the airlines and is categorized into positive, negative and neutral tweets, followed \n",
    "by categorizing negative reasons further such as “delay” or “rude service”. \n",
    "The dataset is processed, and modelling techniques are applied further to get desired results.\n",
    "\n",
    "Natural language processing techniques such as word clouds, bag of words, ngrams, sentiment analysis etc., are used to process the data. \n",
    "Also, machine learning techniques such as logistic regression, random forest, support vector machine, \n",
    "K-Nearest Neighbor, Decision tree are applied to predict the outcome variables. \n",
    "A baseline model, KNN classifier is performed to check the accuracy and use it as a baseline for rest our analysis. \n",
    "Then, we compute the accuracies for various models to recognize the best performing model among the different models we applied. \n",
    "We got the best accuracy for sentiment analysis with (insert method and accuracy).\n",
    "\n",
    "Steps to Be followed:\n",
    "\n",
    "1.Download the folder \"AIT_690_MSY\" and save it on desktop\n",
    "2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Algorithm\n",
    "\n",
    "1.The Program will install all the required libraries and will read the CSV file.\n",
    "\n",
    "2.we clean the dataset by removing unneccessary Columns like negativereason_gold and \n",
    "airline_sentiment_gold are removed.\n",
    "\n",
    "3.As a part of data cleaning some rules are defined which extracts the exact meaning of the word and \n",
    "then we remove special character,remove URL and hashtags and converting words like \"isn't to is not\".\n",
    "\n",
    "4.All the tweets are tokenized and will do lemmatizing for every word in the list, check for stop words and \n",
    "message will be appended to the \"msg_list\".\n",
    "\n",
    "5.Mood of the tweets is assigned to variable X and mood count is assigned to varibale Y, for different airlines to\n",
    "plot and see the distribution.\n",
    "\n",
    "6.We visualize the distribution of counts for all the airlines.\n",
    "\n",
    "7.All the negative tweets are also visualized using word cloud.\n",
    "\n",
    "8.The sentiment is categorized as negative,neutral and positive in the dataset, we assign values to them\n",
    "as 0,1,2 respectively.\n",
    "\n",
    "9.All the clean tweets is assigned to new variable X, All the tweets sentiments are given to variable Y.\n",
    "\n",
    "10.Now, We split the dataset in ratio of 80:20, where as training - 80% and testing - 20%.\n",
    "\n",
    "11.Split the X and Y variables.\n",
    "\n",
    "12.Make a new instance for bag of words model using CountVectorizer and pass parameters for max_features = 1000\n",
    "\n",
    "13.Fit our training data tweets to vector, transforming training and testing data tweets.\n",
    "\n",
    "14.Import SVM and other classifiers and fit the training and test data to the model\n",
    "\n",
    "15.Do 10-fold cross validation on the training data and calculates mean accuracy.\n",
    "\n",
    "16.Predict the sentiments of testing data and calculates the accuracy of predicted data sentiment with original\n",
    "tweet sentiment of test data.\n",
    "\n",
    "17.It iterates from step 15-17 for other classifers.\n",
    "\n",
    "18.We calculate top-10 and last-10 words for the logisitic regression model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "from autocorrect import spell\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.703060e+17           neutral                        1.0000   \n",
       "1  5.703010e+17          positive                        0.3486   \n",
       "2  5.703010e+17           neutral                        0.6837   \n",
       "3  5.703010e+17          negative                        1.0000   \n",
       "4  5.703010e+17          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "   tweet_created tweet_location               user_timezone  \n",
       "0  2/24/15 11:35            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/15 11:15            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/15 11:15      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/15 11:15            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/15 11:14            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../twitter/Tweets.csv\")#Reads the input file from the file location.\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['tweet_coord']#null value columns are deleted.\n",
    "del df['airline_sentiment_gold']\n",
    "del df['negativereason_gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  5.703060e+17           neutral                        1.0000   \n",
       "1  5.703010e+17          positive                        0.3486   \n",
       "2  5.703010e+17           neutral                        0.6837   \n",
       "3  5.703010e+17          negative                        1.0000   \n",
       "4  5.703010e+17          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline        name  \\\n",
       "0            NaN                        NaN  Virgin America     cairdin   \n",
       "1            NaN                     0.0000  Virgin America    jnardino   \n",
       "2            NaN                        NaN  Virgin America  yvonnalynn   \n",
       "3     Bad Flight                     0.7033  Virgin America    jnardino   \n",
       "4     Can't Tell                     1.0000  Virgin America    jnardino   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0              0                @VirginAmerica What @dhepburn said.   \n",
       "1              0  @VirginAmerica plus you've added commercials t...   \n",
       "2              0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3              0  @VirginAmerica it's really aggressive to blast...   \n",
       "4              0  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "   tweet_created tweet_location               user_timezone  \n",
       "0  2/24/15 11:35            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/15 11:15            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/15 11:15      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/15 11:15            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/15 11:14            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decontracted function will expand the english words like haven't to have not.\n",
    "\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"n\\'t\", \"not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \"would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \"not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "list3=[]\n",
    "#it will remove special characters.\n",
    "def  remove_characters(sentence):\n",
    "    y=sentence.split(\" \")\n",
    "    s=' '\n",
    "    list2=[]\n",
    "    for i in y:\n",
    "        i=i.lower()\n",
    "        if re.search('[^a-zA-Z0-9.$]',i):\n",
    "            pass\n",
    "        else:\n",
    "            list2.append(i)\n",
    "    x=s.join(list2)\n",
    "    list3.append(x)\n",
    "    return x\n",
    "# it will remove the URL from the tweets\n",
    "def url(msg):\n",
    "    msg= re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', msg)\n",
    "    return msg\n",
    "\n",
    "##def sentimentsc(words):\n",
    "    ##sentimentScore = 0\n",
    "    #for word in words.split():        \n",
    "     #   ind = sentimentDictionary[(sentimentDictionary['word'] == word)].index.tolist()\n",
    "       # if len(ind) != 0:\n",
    "      #      sentimentScore = sentimentScore + float(sentimentDictionary.loc[ind]['score'])\n",
    "    #return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()# it will remove the URL from the tweets\n",
    "lemmatizer = WordNetLemmatizer()#lemmatizer function is defined.\n",
    "stwords = stopwords.words('english')#stopwords function is defined and calling the function.\n",
    "#stwords.remove('not')\n",
    "#stwords.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=df['text']# tweet is called into the variable d.\n",
    "corpus=[]\n",
    "null_index=[]\n",
    "\n",
    "#sentimentDictionary = pd.read_table('../twitter/AFINN-111.txt',sep=',', names = ['word','score'])\n",
    "#\n",
    "#sentimentDictionary = pd.read_table('sentiword.txt',sep=',', names = ['word','score'])\n",
    "#\n",
    "#c=sentimentDictionary['word'].str.split('#',n=3)\n",
    "#\n",
    "#d=sentimentDictionary['word'].str.split('\\t',n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_list = []# empty list is passed.\n",
    "for i in range(0,len(d)):  \n",
    "    msg=str(d[i])\n",
    "    msg = decontracted(msg) # decontracted function is called here.    \n",
    "    msg = msg.lower()  #tweets are converted to lower case.  \n",
    "    msg = remove_characters(msg)# it will remove the special characters \n",
    "    msg = url(msg)#it will remove the url from the tweets.          \n",
    "    msg = word_tokenize(msg)# tweet is tokenized here.\n",
    "    msg = [i2 for i2 in msg if i2.isalpha()]    \n",
    "    msg = [lemmatizer.lemmatize(word, pos= \"a\") for word in msg if not word in set(stwords) ]\n",
    "    msg = [spell(word) for word in msg]\n",
    "    msg = [word for word in msg if not word in set(stwords)]\n",
    "    msg = ' '.join(msg)\n",
    "    #print(msg)\n",
    "    msg_list.append(msg)\n",
    "#print(msg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df = pd.DataFrame(clean_df1,columns=['text'])\n",
    "clean_msg = pd.DataFrame(msg_list,columns=['text'])\n",
    "clean_msg.head()\n",
    "#print(clean_msg)\n",
    "# tweets are passed to variable X.\n",
    "X=clean_msg.text\n",
    "#sentiments are assigned the label negative, neutral and positive as 0,1,2 respectively\n",
    "df['sentiment'] = df['airline_sentiment'].apply(lambda x: 0 if x=='negative' else (1 if x== 'neutral' else 2))\n",
    "Y=df.sentiment#tweets sentiment labels are assigned to variable Y.\n",
    "#print(Y)\n",
    "#print(Y.shape)\n",
    "#print(X)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import countvectorizer from sklearn feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bowvector = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bowvector.fit_transform(msg_list)\n",
    "#bow\n",
    "X1=bow.toarray()\n",
    "#print(X1)\n",
    "#len(bowvector.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is splitted into training and testing data with the ratio of 80:20 percent respectively.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X1,Y,test_size=0.2,random_state=42)\n",
    "#print(X_train)\n",
    "#print(test)\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=19,\n",
      "  shrinking=True, tol=0.001, verbose=False) 0.6454918032786885\n",
      "[[1888    0    1]\n",
      " [ 580    0    0]\n",
      " [ 457    0    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.78      1889\n",
      "           1       0.00      0.00      0.00       580\n",
      "           2       0.67      0.00      0.01       459\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      2928\n",
      "   macro avg       0.44      0.33      0.26      2928\n",
      "weighted avg       0.52      0.65      0.51      2928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC#imports SVM classifier from sklearn.svm\n",
    "classifier = SVC(kernel = 'rbf', random_state =19 )\n",
    "classifier.fit(X_train, Y_train)\n",
    "#prediction\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print(classifier,accuracy_score(Y_test,Y_pred))\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred) \n",
    "print(cm)\n",
    "#import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6230359929375694"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=8,\n",
      "            splitter='random') 0.680327868852459\n",
      "[[1457  296  136]\n",
      " [ 218  285   77]\n",
      " [ 125   84  250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1889\n",
      "           1       0.43      0.49      0.46       580\n",
      "           2       0.54      0.54      0.54       459\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2928\n",
      "   macro avg       0.59      0.60      0.60      2928\n",
      "weighted avg       0.69      0.68      0.69      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "treeclassifier = DecisionTreeClassifier(criterion = 'gini', random_state = 8, splitter = 'random')\n",
    "treeclassifier.fit(X_train, Y_train)\n",
    "#prediction\n",
    "Y_pred = treeclassifier.predict(X_test)\n",
    "print(treeclassifier,accuracy_score(Y_test,Y_pred))\n",
    "\n",
    "\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred) \n",
    "print(cm)\n",
    "#import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6660666041554377"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = treeclassifier, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') 0.6075819672131147\n",
      "[[1169  609  111]\n",
      " [ 160  366   54]\n",
      " [  93  122  244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71      1889\n",
      "           1       0.33      0.63      0.44       580\n",
      "           2       0.60      0.53      0.56       459\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      2928\n",
      "   macro avg       0.58      0.59      0.57      2928\n",
      "weighted avg       0.69      0.61      0.63      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "kclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "kclassifier.fit(X_train, Y_train)\n",
    "#prediction\n",
    "Y_pred = kclassifier.predict(X_test)\n",
    "print(kclassifier,accuracy_score(Y_test,Y_pred))\n",
    "\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred) \n",
    "print(cm)\n",
    "#import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960648677259519"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = kclassifier, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=19, verbose=0, warm_start=False) 0.7278005464480874\n",
      "[[1580  195  114]\n",
      " [ 217  284   79]\n",
      " [ 111   81  267]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83      1889\n",
      "           1       0.51      0.49      0.50       580\n",
      "           2       0.58      0.58      0.58       459\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2928\n",
      "   macro avg       0.64      0.64      0.64      2928\n",
      "weighted avg       0.73      0.73      0.73      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "ranclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 19)\n",
    "ra = ranclassifier.fit(X_train, Y_train)\n",
    "#prediction\n",
    "Y_pred = ranclassifier.predict(X_test)\n",
    "print(ranclassifier,accuracy_score(Y_test,Y_pred))\n",
    "\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred)          \n",
    "print(cm)\n",
    "\n",
    "#import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7062833384469733"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = ranclassifier, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=19, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) 0.7752732240437158\n",
      "[[1683  152   54]\n",
      " [ 229  308   43]\n",
      " [ 108   72  279]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1889\n",
      "           1       0.58      0.53      0.55       580\n",
      "           2       0.74      0.61      0.67       459\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2928\n",
      "   macro avg       0.72      0.68      0.69      2928\n",
      "weighted avg       0.77      0.78      0.77      2928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logclassifier = LogisticRegression(random_state = 19, penalty = 'l1')\n",
    "lr=logclassifier.fit(X_train, Y_train)\n",
    "#prediction\n",
    "Y_pred = logclassifier.predict(X_test)\n",
    "print(logclassifier,accuracy_score(Y_test,Y_pred))\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred) \n",
    "print(cm)\n",
    "\n",
    " #import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lakshmi_shetty/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7427402613515612"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = logclassifier, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True) 0.7359972677595629\n",
      "[[1660  154   75]\n",
      " [ 304  223   53]\n",
      " [ 144   43  272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83      1889\n",
      "           1       0.53      0.38      0.45       580\n",
      "           2       0.68      0.59      0.63       459\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2928\n",
      "   macro avg       0.67      0.62      0.64      2928\n",
      "weighted avg       0.72      0.74      0.72      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "nbmodel=MultinomialNB(alpha=0.5)\n",
    "nbmodel=nbmodel.fit(X_train,Y_train)\n",
    "Y_pred=nbmodel.predict(X_test)\n",
    "print(nbmodel,accuracy_score(Y_test,Y_pred))\n",
    "#imports the confusion matrix from sklearn metrics.\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_test, Y_pred) \n",
    "print(cm)\n",
    "#import classification report where it will print the precision recall and support.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7216539418177245"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # import cross_val_score from sklear.model_selection\n",
    "accuracies = cross_val_score(estimator = nbmodel, X = X_train, y = Y_train, cv = 10) \n",
    "#K- fold cross validation on our traing data and its sentimenst with 10 fold cross validation\n",
    "accuracies.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ten         coeff          word\n",
      "759  3.189536       screwed\n",
      "729  3.176618    ridiculous\n",
      "326  3.172009           fix\n",
      "348  3.035406        forced\n",
      "935  2.942165       useless\n",
      "833  2.908148      suitcase\n",
      "737  2.657218          rude\n",
      "969  2.581241        werent\n",
      "421  2.517903       holding\n",
      "918  2.424534  unacceptable\n",
      "last ten         coeff       word\n",
      "223 -2.159570      deals\n",
      "953 -2.195719       warm\n",
      "625 -2.250058   passbook\n",
      "73  -2.257408    awesome\n",
      "943 -2.259366      visit\n",
      "978 -2.274019  wonderful\n",
      "479 -2.359816      kudos\n",
      "864 -2.441605      thank\n",
      "286 -2.630345  excellent\n",
      "987 -3.374083    worries\n"
     ]
    }
   ],
   "source": [
    "#imports countvectorier and TFIDvectorizer from sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#gives top-10 and last-10 words for the logistic regression model\n",
    "words= bowvector.get_feature_names()\n",
    "co=lr.coef_.tolist()[0]\n",
    "coeff=pd.DataFrame({'word':words,'coeff':co})\n",
    "coeffdf=coeff.sort_values(['coeff','word'],ascending=[0,1])\n",
    "print('top ten',coeffdf.head(10))\n",
    "print('last ten',coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
